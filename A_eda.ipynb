{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "In this notebook, we will be importing the data and conducting EDA to evaluate our most pertinent features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Initialization\n",
    "\n",
    "Importing libraries and setting seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting PYTHONHASHSEED\n",
    "import os\n",
    "\n",
    "pyhashseed1 = os.environ.get('PYTHONHASHSEED')\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "pyhashseed2 = os.environ.get('PYTHONHASHSEED')\n",
    "\n",
    "# NOTEBOOK EXCLUSIVE CODE\n",
    "if __name__ == \"__main__\":\n",
    "    print('Make sure the following says \\'None\\': ', pyhashseed1)\n",
    "    print('Make sure the following says \\'0\\': ', pyhashseed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# Setting seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMAGES_PATH = Path() / 'plots'\n",
    "\n",
    "def save_fig(fig_name, tight_layout=True, fig_extension='png', resolution=300):\n",
    "    '''Saves an image to the plots folder with the specified name.'''\n",
    "    path = IMAGES_PATH / f'{fig_name}.{fig_extension}'\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_show(plt_name):\n",
    "    '''Saves an image using save_fig() under the plt_name and displays it.'''\n",
    "    save_fig(plt_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data\n",
    "adclicks = pd.read_csv('data/ad_click_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Overview of data structure\n",
    "\n",
    "We will be examining the data and its underlying structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining the top 10 rows\n",
    "adclicks.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTEBOOK EXCLUSIVE CODE\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Checking duplicates\n",
    "    duplicates = adclicks.duplicated()\n",
    "    print(len(adclicks[duplicates]))\n",
    "    \n",
    "# Dropping duplicates\n",
    "adclicks = adclicks.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTEBOOK EXCLUSIVE CODE\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Information on features\n",
    "    adclicks.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see that there are quite a bit of missing information (NaNs) and we also had to remove a significant amount of duplicate data.\n",
    "\n",
    "A summary of `adclicks.info()`:\n",
    "\n",
    "- There are 7,147 instances.\n",
    "- `age`, `gender`, `device_type`, `ad_position`, `browsing_history`, and `time_of_day` have missing values.\n",
    "- Most of the features are non-numeric, possibly categorical.\n",
    "- `full_name` seems to be an irrelevant feature, as all the names are anonymized. I will remove this feature.\n",
    "    - I will also check if `id` is a unique identifier, if so, I will drop it as use rowID as an identifier instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTEBOOK EXCLUSIVE CODE\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # The number of unique IDs\n",
    "    print(adclicks['id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `id` is not unique for all rows, we will keep it and keep in mind this fact when we split our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping `full_name`\n",
    "adclicks = adclicks.drop(columns='full_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Examining categorical features\n",
    "\n",
    "Let's examine what the possible values our non-numerical features have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_features(df):\n",
    "    '''Returns a list of the names of the cateogorical columns in a pandas dataframe.'''\n",
    "    return list(df.select_dtypes(include=['object', 'category', 'bool']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible values of the categorical features\n",
    "categoricals = categorical_features(adclicks)\n",
    "\n",
    "# NOTEBOOK EXCLUSIVE CODE\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Check the columns in categoricals\n",
    "    display(categoricals)\n",
    "\n",
    "    # Display the unique values for each categorical feature\n",
    "    for feature in categoricals:\n",
    "        print(f'{feature}: ' ,list(adclicks[feature].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each categorical feature represents the following:\n",
    "\n",
    "- `gender`: the gender of the user (Male, Female, Non-Binary).\n",
    "- `device_type`: the type of device of the user when viewing the ad (Mobile, Desktop, Tablet).\n",
    "- `ad_position`: the position of the ad on the webpage (Top, Side, Bottom).\n",
    "- `browsing_history`: the user's browsing activity prior to seeing the ad (Shopping, News, Entertainment, Education, Social Media).\n",
    "- `time_of_day`: the time when the user viewed the ad (Morning, Afternoon, Evening, Night).\n",
    "\n",
    "Our numerical features represent the following:\n",
    "\n",
    "- `id`: unique ID for each user (NOT EACH ROW).\n",
    "    - This is important to note as we want to be aware of this when we split our test set in order to avoid data leakage.\n",
    "- `age`: the age of each user.\n",
    "- `click`: the **target label** indicating whether the user clicked on the ad (Binary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Splitting an initial training set for EDA + transformations\n",
    "\n",
    "We will be splitting an initial training set to evaluate for EDA and assess what the necessary transformations will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting a random train-test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = adclicks.drop(columns='click')\n",
    "y = adclicks['click']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Pre-split EDA\n",
    "\n",
    "We will perform some light EDA before we split to understand the overall pattern of the data and how it compares to our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTEBOOK EXCLUSIVE CODE\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Display value counts for categorical features\n",
    "    print('VALUE COUNTS OF CATEGORICAL FEATURES')\n",
    "    for category in categoricals:\n",
    "        display(X_train[category].value_counts())\n",
    "        \n",
    "    # Description statistics\n",
    "    print('\\nDESCRIPTION STATISTICS')\n",
    "    display(X_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "\n",
    "# NOTEBOOK EXCLUSIVE CODE\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Evaluate missing data\n",
    "    missing_values = X_train[X_train.isnull().any(axis=1)]\n",
    "    print(len(missing_values))\n",
    "    \n",
    "    # Show the missing matrix\n",
    "    msno.matrix(X_train)\n",
    "    plt_show('missing_matrix')\n",
    "    \n",
    "    # Show the nullity correlation heatmap\n",
    "    msno.heatmap(X_train)\n",
    "    plt_show('nullity_corr_heatmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a small look at our data, the following can be observed:\n",
    "\n",
    "- The classes of each categorical feature are balanced\n",
    "- The average age is 40 years old, and the data seems to not be skewed as the median also aligns with this value\n",
    "- Most of the instances have missing data, which will significantly affect our modeling.\n",
    "    - Luckily, the data shows to be missing at random (MAR), based on the sporadic pattern of the missing matrix and the neutral nullity correlation heatmap, so we can be optimistic about not running into issues during imputation of missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Splitting the test set\n",
    "\n",
    "Considering that our target variable `click` is categorical, I am thinking that a **stratified-split** would be the best way to split our training and test data in order to represent our classes well. We will also have to split based on `id` to ensure that the same user is not in both the training and test set.\n",
    "\n",
    "First, I want to see if we have any class imbalances in the `id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTEBOOK EXCLUSIVE CODE\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Check the balance of user id\n",
    "    max_appearance = adclicks['id'].value_counts().max()\n",
    "    display(max_appearance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most that any user appears is 14 times, which is not bad news for our splitting. We will proceed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting our data\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "X = adclicks.drop(columns=['click']).copy()\n",
    "y = adclicks['click'].copy()\n",
    "groups = adclicks['id']\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=False)\n",
    "# Setting shuffle to false so that the indices stay consistent in the split\n",
    "folds = []\n",
    "\n",
    "for train_index, test_index in sgkf.split(X, y, groups=groups):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    folds.append([X_train, X_test, y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTEBOOK EXCLUSIVE CODE\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Checking that our split is sound\n",
    "    for fold, i in zip(folds, range(0, len(folds))):\n",
    "        X_train, X_test, y_train, y_test = fold\n",
    "        \n",
    "        train_users = set(X_train['id'])\n",
    "        test_users = set(X_test['id'])\n",
    "        \n",
    "        # Printing output\n",
    "        print(f'No overlap in train and test users in split {i + 1}:', train_users.isdisjoint(test_users))\n",
    "        \n",
    "        print('Train label values:')\n",
    "        print(y_train.unique())\n",
    "        print('Test label values:')\n",
    "        print(y_test.unique(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have no overlap in our test and train users and our train and test sets both have representation of our target variable `click`. Just as a precaution for when we implement our models later, I am going to calculate the \"sample weight\" of each user ID in the data (below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating sample weight in the training data\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "for fold in folds:\n",
    "    train_set = fold[0].copy()\n",
    "    sample_weight = compute_sample_weight(class_weight='balanced', y=train_set['id'])\n",
    "    train_set.loc[:, 'sample_weight'] = sample_weight\n",
    "    fold[0] = train_set\n",
    "\n",
    "# NOTEBOOK EXCLUSIVE CODE\n",
    "if __name__ == \"__main__\":\n",
    "    # Display for the first split\n",
    "    display(folds[0][0].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Next steps\n",
    "\n",
    "Now, with successfully split data, we will move on to transforming and preprocessing. While EDA would usually occur during this time, since we split our training data into multiple splits, there's a chance that evaluating each split would result in data leakage. In order to avoid this, we will just proceed and apply any imputations that were fit to the original random training set that we split for EDA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
