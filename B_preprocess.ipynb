{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "In this notebook, we will be preprocessing the data for machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Import libraries + Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make sure the following says None:  None\n",
      "Make sure the following says 0:  0\n"
     ]
    }
   ],
   "source": [
    "# Setting PYTHONHASHSEED\n",
    "import os\n",
    "print('Make sure the following says None: ', os.environ.get('PYTHONHASHSEED'))\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "print('Make sure the following says 0: ', os.environ.get('PYTHONHASHSEED'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "import missingno as msno\n",
    "\n",
    "# Setting seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# To importing script\n",
    "sys.path.append('./py_scripts')\n",
    "import A_eda as A\n",
    "\n",
    "# Importing functions from A\n",
    "from A_eda import plt_show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data + variables\n",
    "adclicks = A.adclicks\n",
    "test_set = A.test_set\n",
    "categoricals = A.categoricals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Data Cleaning\n",
    "\n",
    "We will be performing necessary data transformations and handling missing data in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking all the missing values\n",
    "missing_values = adclicks[adclicks.isnull().any(axis=1)]\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all of our rows (7361 out of 8000) are missing one or more features. A possible method of imputation is the MICE (Multivariate Imputation of Chained Equations), but it is best suited for data that is MAR (Missing At Random). Let us check this assumption below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Imputing with MICE\n",
    "\n",
    "Next, we will be imputing our missing values using the MICE method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode each categorical column (training and testing)\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def encode_training(df, encode_columns):\n",
    "    encoders = {col: preprocessing.LabelEncoder() for col in encode_columns}\n",
    "    labels = {}\n",
    "\n",
    "    # Encoding for each categorical variable and joining in new dataset\n",
    "    for col, encoder in encoders.items():\n",
    "        encode_col = f'{col}_en'\n",
    "        encoded_col = encoder.fit_transform(df.loc[:, col])\n",
    "        \n",
    "        # Print label mapping\n",
    "        labels[col] = (dict(zip(encoder.classes_, encoder.transform(encoder.classes_))))\n",
    "        print(labels[col])\n",
    "        df[encode_col] = encoded_col\n",
    "    \n",
    "    return encoders, labels\n",
    "\n",
    "def encode_test(df, encoders):\n",
    "    for col, encoder in encoders.items():\n",
    "        encode_col = f'{col}_en'\n",
    "        encoded_col = encoder.transform(df.loc[:, col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode training set + test set\n",
    "encoders, labels = encode_training(adclicks, categoricals)\n",
    "adclicks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make NaN equivalent to NaN agains\n",
    "for col in encoders:\n",
    "    col_labels = labels[col]\n",
    "    encode_col = f'{col}_en'\n",
    "    adclicks[encode_col] = adclicks[encode_col].map(lambda x: np.nan if x == col_labels[np.nan] else x)\n",
    "    \n",
    "adclicks.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing with IterativeImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "imputed_df = imputer.fit_transform(adclicks)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
