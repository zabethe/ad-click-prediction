{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ad Click Prediction\n",
    "\n",
    "In this notebook, we will be analyzing ad click data from [Kaggle](https://www.kaggle.com/datasets/marius2303/ad-click-prediction-dataset) and building a prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Initialization\n",
    "\n",
    "Importing libraries, data etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting PYTHONHASHSEED\n",
    "import os\n",
    "\n",
    "pyhashseed1 = os.environ.get(\"PYTHONHASHSEED\")\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "pyhashseed2 = os.environ.get(\"PYTHONHASHSEED\")\n",
    "\n",
    "# NOTEBOOK EXCLUSIVE CODE\n",
    "if __name__ == \"__main__\":\n",
    "    print('Make sure the following says \\'None\\': ', pyhashseed1)\n",
    "    print('Make sure the following says \\'0\\': ', pyhashseed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# Setting seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying + saving plot function\n",
    "IMAGES_PATH = Path() / \"plots\"\n",
    "\n",
    "def save_fig(fig_name, tight_layout=True, fig_extension='png', resolution=300):\n",
    "    '''Saves an image to the plots folder with the specified name.'''\n",
    "    path = IMAGES_PATH / f'{fig_name}.{fig_extension}'\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "def plot_show(plt_name):\n",
    "    '''Saves an image using save_fig() under the plt_name and displays it.'''\n",
    "    save_fig(plt_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "adclicks = pd.read_csv(\"data/ad_click_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# EDA + Splitting Data\n",
    "\n",
    "This section will be a comprehensive analysis and cleaning of the data. We will examine its structure, remove redundant data, and examine relationships. We will also split the data into the training and test set.\n",
    "\n",
    "## Data Overview\n",
    "\n",
    "Structure of the dataset, % of missing/duplicate values etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a quick snapshot\n",
    "adclicks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate duplicate values\n",
    "duplicates = adclicks.duplicated()\n",
    "print(\"There are\", len(adclicks[duplicates]), \"duplicate rows.\")\n",
    "\n",
    "# Drop duplicates\n",
    "adclicks = adclicks.drop_duplicates()\n",
    "print(\"The duplicate rows have been dropped. There are\", len(adclicks), \"rows remaining.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General information\n",
    "adclicks.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning + Data Visualization\n",
    "\n",
    "Evaluate missing values and think about how to deal with them. Visualize distributions of features and conduct initial visual analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data\n",
    "\n",
    "Examining missing values and thinking about how to deal with the missingness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many missing values in each feature?\n",
    "missing = adclicks.isna().sum()\n",
    "display(missing)\n",
    "\n",
    "# How many rows with missing values?\n",
    "missing_rows_count = adclicks.isnull().any(axis=1).sum()\n",
    "print(f\"Number of rows with missing data: {missing_rows_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "\n",
    "# Check pattern of missingness\n",
    "\n",
    "# Missing matrix\n",
    "msno.matrix(adclicks)\n",
    "plot_show(\"missing_matrix\")\n",
    "\n",
    "# Nullity correlation heatmap\n",
    "msno.heatmap(adclicks)\n",
    "plot_show(\"nullity_corr_heatmap\")\n",
    "\n",
    "# Dendrogram\n",
    "msno.dendrogram(adclicks)\n",
    "plot_show(\"missing_dendrogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for missingness\n",
    "missing_corr = adclicks.isnull().corr()\n",
    "sns.heatmap(missing_corr, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation of Missingness\")\n",
    "plot_show(\"missing_correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining Unique Values\n",
    "\n",
    "Examine the unique values of the categorical features and if any of the columns can serve as a row ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract categorical columns\n",
    "categorical = adclicks.select_dtypes(include=[\"object\", \"category\", \"bool\"])\n",
    "\n",
    "# Display unique values in each categorical column\n",
    "for feature in categorical:\n",
    "    print(f\"{feature}: \" ,list(categorical[feature].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique number of `id` and `full_name`\n",
    "print(adclicks[\"id\"].nunique())\n",
    "print(adclicks[\"full_name\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping `full_name`\n",
    "adclicks = adclicks.drop(columns=\"full_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Recurring Users\n",
    "\n",
    "Since we have recurring users, we are going to quickly examine their effect on the data to determine the best method of splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract recurring users\n",
    "# Count `id` occurrences\n",
    "id_counts = adclicks[\"id\"].value_counts()\n",
    "print(f\"The maximum number of times a user occurs is {id_counts.max()}.\")\n",
    "\n",
    "# Separate recurring users and single users\n",
    "recurring_ids = id_counts[id_counts > 1].index\n",
    "recurring_users = adclicks[adclicks[\"id\"].isin(recurring_ids)]\n",
    "single_users = adclicks[~adclicks[\"id\"].isin(recurring_ids)]\n",
    "\n",
    "# Count occurrences of each\n",
    "print(f\"Total number of recurring users: {recurring_users[\"id\"].nunique()}\")\n",
    "print(f\"Total number of single users: {single_users[\"id\"].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe each dataset\n",
    "print(\"Single User Statistics\")\n",
    "display(single_users.describe())\n",
    "print(\"Recurring User Statistics\")\n",
    "display(recurring_users.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Categorical Features\n",
    "\n",
    "We will examine the distribution of the whole dataset and the users by category of occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create plot grid of barplots\n",
    "def plot_barplots(dataframes, dataframe_names, features, colors):\n",
    "    '''A function that outputs a grid of barplots.'''\n",
    "    \n",
    "    num_df = len(dataframes)\n",
    "    num_features = len(features)\n",
    "    \n",
    "    # Each figure will be 6 by 4\n",
    "    fig, axes = plt.subplots(num_features, num_df, figsize=(6*num_df, 4*num_features), sharey=True)\n",
    "    \n",
    "    # Iterate through and plot figures\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        # Gather data\n",
    "        df = dataframes[i % num_df]\n",
    "        df_name = dataframe_names[i % num_df]\n",
    "        feature = features[i // num_df]\n",
    "        feature_name = feature.capitalize()\n",
    "        color = colors[i // num_df]\n",
    "        \n",
    "        # Configure data for sns\n",
    "        counts = df[feature].value_counts(dropna=False).reset_index()\n",
    "        counts.columns = [feature_name, \"Count\"]\n",
    "        counts[feature_name] = counts[feature_name].fillna(\"Missing\")   # Convert Na's to Missing\n",
    "        \n",
    "        # Create barplot\n",
    "        sns.barplot(x=feature_name, y=\"Count\", data=counts, color=color, ax=ax)\n",
    "        \n",
    "        # Extra plot details\n",
    "        ax.set_title(f\"{feature_name} Counts for {df_name}\")\n",
    "        ax.set_xlabel(feature_name)\n",
    "        ax.set_ylabel(\"Count\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [adclicks, single_users, recurring_users]\n",
    "dataframe_names = [\"Whole Data\", \"Singly Occurring Users\", \"Recurring Users\"]\n",
    "features = list(set(categorical).intersection(set(adclicks.columns)))\n",
    "colors = [\"skyblue\", \"orange\", \"purple\", \"green\", \"red\"]\n",
    "\n",
    "plot_barplots(dataframes, dataframe_names, features, colors)\n",
    "plot_show(\"categorical_barplots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions of Numerical Features\n",
    "\n",
    "Evaluating the distribution of numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information of numerical features\n",
    "adclicks.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create plot grid of boxplots\n",
    "def plot_boxplots(dataframes, dataframe_names, features, colors, group_by=None):\n",
    "    '''A function that outputs a grid of boxplots.'''\n",
    "    \n",
    "    num_df = len(dataframes)\n",
    "    num_features = len(features)\n",
    "    \n",
    "    # Each figure will be 6 by 4\n",
    "    fig, axes = plt.subplots(num_features, num_df, figsize=(6*num_df, 4*num_features), sharey=True)\n",
    "    \n",
    "    # Iterate through and plot figures\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        # Gather data\n",
    "        df = dataframes[i % num_df]\n",
    "        df_name = dataframe_names[i % num_df]\n",
    "        feature = features[i // num_df]\n",
    "        feature_name = feature.capitalize()\n",
    "        color = colors[i // num_df]\n",
    "        \n",
    "        # Create barplot\n",
    "        sns.boxplot(x=group_by, y=feature, data=df, color=color, ax=ax)\n",
    "        \n",
    "        # Extra plot details\n",
    "        ax.set_title(f\"{feature_name} Boxplot for {df_name}\")\n",
    "        ax.set_xlabel(feature_name)\n",
    "        ax.set_ylabel(group_by)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of age with user category by occurrence\n",
    "features = [\"age\"]\n",
    "colors = [\"lightblue\"]\n",
    "\n",
    "plot_boxplots(dataframes, dataframe_names, features, colors)\n",
    "plot_show(\"age_boxplots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender vs Age\n",
    "plot_boxplots(dataframes, dataframe_names, features, [\"purple\"], group_by=\"gender\")\n",
    "plot_show(\"age_vs_gender_boxplots\")\n",
    "\n",
    "# Ad Position vs Age\n",
    "plot_boxplots(dataframes, dataframe_names, features, [\"orange\"], group_by=\"ad_position\")\n",
    "plot_show(\"age_vs_ad_position_boxplots\")\n",
    "\n",
    "# Device Type vs Age\n",
    "plot_boxplots(dataframes, dataframe_names, features, [\"skyblue\"], group_by=\"device_type\")\n",
    "plot_show(\"age_vs_device_type_boxplots\")\n",
    "\n",
    "# Browsing History vs Age\n",
    "plot_boxplots(dataframes, dataframe_names, features, [\"green\"], group_by=\"browsing_history\")\n",
    "plot_show(\"age_vs_browsing_history_boxplots\")\n",
    "\n",
    "# Time of Day vs Age\n",
    "plot_boxplots(dataframes, dataframe_names, features, [\"red\"], group_by=\"time_of_day\")\n",
    "plot_show(\"age_vs_time_of_day_boxplots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create plot grid of histograms\n",
    "def plot_histograms(dataframes, dataframe_names, features, colors):\n",
    "    '''A function that outputs a grid of histograms.'''\n",
    "    \n",
    "    num_df = len(dataframes)\n",
    "    num_features = len(features)\n",
    "    \n",
    "    # Each figure will be 6 by 4\n",
    "    fig, axes = plt.subplots(num_features, num_df, figsize=(6*num_df, 4*num_features), sharey=True)\n",
    "    \n",
    "    # Iterate through and plot figures\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        # Gather data\n",
    "        df = dataframes[i % num_df]\n",
    "        df_name = dataframe_names[i % num_df]\n",
    "        feature = features[i // num_df]\n",
    "        feature_name = feature.capitalize()\n",
    "        color = colors[i // num_df]\n",
    "        \n",
    "        # Create barplot\n",
    "        sns.histplot(x=feature, data=df, kde=True, color=color, ax=ax)\n",
    "        \n",
    "        # Extra plot details\n",
    "        ax.set_title(f\"{feature_name} Histogram for {df_name}\")\n",
    "        ax.set_xlabel(feature_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of age with user category by occurrence\n",
    "features = [\"age\"]\n",
    "colors = [\"blue\"]\n",
    "\n",
    "plot_histograms(dataframes, dataframe_names, features, colors)\n",
    "plot_show(\"age_histograms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Preprocessing Before Splitting\n",
    "\n",
    "Preprocess data by imputing and encoding in multiple ways for different types of models.\n",
    "\n",
    "## Aggregating and Collapsing Recurring User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate and collapse recurring user data\n",
    "recurring_users = recurring_users.fillna(\"Missing\")\n",
    "\n",
    "# Add `num_visits` column to represent the number of times a user visited\n",
    "recurring_users[\"num_visits\"] = recurring_users[\"id\"].map(recurring_users[\"id\"].value_counts())\n",
    "single_users.loc[:,\"num_visits\"] =  1\n",
    "\n",
    "recurring_users_collapsed = recurring_users.groupby(\"id\").agg(lambda x: list(set(x).difference({\"Missing\"})))\n",
    "recurring_users_collapsed = recurring_users_collapsed.map(lambda x: x[0] if isinstance(x, list) and x else np.nan).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add `recurring_user` column to both recurring and singly occuring users\n",
    "recurring_users_collapsed.loc[:,\"recurring_user\"] = 1\n",
    "single_users.loc[:,\"recurring_user\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data\n",
    "adclicks_users = pd.concat([single_users, recurring_users_collapsed], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop `id`\n",
    "\n",
    "Since we know each row is a unique user, we can drop `id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop `id`\n",
    "adclicks_users = adclicks_users.drop(columns=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Datasets: Imputation vs. Missing category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation dataset\n",
    "adclicks2 = adclicks_users\n",
    "\n",
    "# Complete dataset\n",
    "adclicks3 = adclicks_users[adclicks_users.columns.difference([\"age\"])].fillna(\"Missing\")\n",
    "adclicks3.loc[:, \"age\"] = adclicks_users[[\"age\"]]\n",
    "reorder = [\"age\", \"gender\", \"device_type\", \"ad_position\", \"browsing_history\", \"time_of_day\", \"click\", \"num_visits\", \"recurring_user\"]\n",
    "adclicks3 = adclicks3[reorder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Splitting data\n",
    "\n",
    "Split the data with respect to the class imbalance of `click`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Split `adclicks2` and then separate indices from `adclicks3`\n",
    "adclicks2_majority = adclicks2[adclicks2.click == 0]\n",
    "adclicks2_minority = adclicks2[adclicks2.click == 1]\n",
    "\n",
    "adclicks2_majority_ds = resample(adclicks2_majority, replace=False, n_samples=len(adclicks2_minority), random_state=42)\n",
    "adclicks2_ds = pd.concat([adclicks2_majority_ds, adclicks2_minority])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract same indices from `adclicks3`\n",
    "adclicks3_ds = adclicks3.loc[adclicks2_ds.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X2 = adclicks3_ds.drop(columns=\"click\")\n",
    "y2 = adclicks3_ds[\"click\"]\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract same indices from `adclicks3_ds`\n",
    "X3 = adclicks3_ds.drop(columns=\"click\")\n",
    "y3 = adclicks3_ds[\"click\"]\n",
    "\n",
    "X3_train = X3.loc[X2_train.index]\n",
    "X3_test = X3.loc[X2_test.index]\n",
    "y3_train = y3.loc[y2_train.index]\n",
    "y3_test = y3.loc[y2_test.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking class imbalanace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create plot grid of barplots\n",
    "def plot_barplots2(dataframes, dataframe_names, features, colors):\n",
    "    '''A function that outputs a grid of barplots.'''\n",
    "    \n",
    "    num_df = len(dataframes)\n",
    "    num_features = len(features)\n",
    "    \n",
    "    # Each figure will be 6 by 4\n",
    "    fig, axes = plt.subplots(num_features, num_df, figsize=(6*num_df, 4*num_features), sharey=True)\n",
    "    \n",
    "    # Iterate through and plot figures\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        # Gather data\n",
    "        df = dataframes[i % num_df]\n",
    "        df_name = dataframe_names[i % num_df]\n",
    "        feature = features[i // num_df]\n",
    "        feature_name = feature.capitalize()\n",
    "        color = colors[i // num_df]\n",
    "        \n",
    "        # Configure data for sns\n",
    "        counts = df[feature].value_counts(dropna=False).reset_index()\n",
    "        counts.columns = [feature_name, \"Count\"]\n",
    "        counts[feature_name] = counts[feature_name].fillna(\"Missing\")   # Convert Na's to Missing\n",
    "        \n",
    "        # Create barplot\n",
    "        sns.barplot(x=feature_name, y=\"Count\", data=counts, color=color, ax=ax)\n",
    "        \n",
    "        # Extra plot details\n",
    "        ax.set_title(f\"{feature_name} Counts for {df_name}\")\n",
    "        ax.set_xlabel(feature_name)\n",
    "        ax.set_ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking class imbalance\n",
    "series = [y2_train, y2_test]\n",
    "series_names = [\"Training data\", \"Testing data\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6*2, 4*1), sharey=True)\n",
    "\n",
    "# Iterate through and plot figures\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    # Gather data\n",
    "    srs = series[i % 2]\n",
    "    srs_name = series_names[i % 2]\n",
    "    feature_name = \"Click\"\n",
    "    color = \"blue\"\n",
    "    \n",
    "    # Configure data for sns\n",
    "    counts = srs.value_counts(dropna=False).reset_index()\n",
    "    counts.columns = [feature_name, \"Count\"]\n",
    "    counts[feature_name] = counts[feature_name].fillna(\"Missing\")   # Convert Na's to Missing\n",
    "    \n",
    "    # Create barplot\n",
    "    sns.barplot(x=feature_name, y=\"Count\", data=counts, color=color, ax=ax)\n",
    "    \n",
    "    # Extra plot details\n",
    "    ax.set_title(f\"{feature_name} Counts for {srs_name}\")\n",
    "    ax.set_xlabel(feature_name)\n",
    "    ax.set_ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Preprocessing after splitting\n",
    "\n",
    "Preprocess by imputing missing data, encoding categorical variables, scaling numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder\n",
    "\n",
    "from feature_engine.imputation import RandomSampleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Pipeline\n",
    "num_transformer1 = Pipeline(\n",
    "    steps =[\n",
    "        (\"imputed\", RandomSampleImputer(random_state=42))\n",
    "    ]\n",
    ")\n",
    "\n",
    "num_transformer2 = Pipeline(\n",
    "    steps =[\n",
    "        (\"imputed\", RandomSampleImputer(random_state=42)),\n",
    "        (\"scaled\", MinMaxScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor1 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"encoding\", OneHotEncoder(), [\"gender\", \"device_type\", \"browsing_history\", \"ad_position\", \"time_of_day\"]),\n",
    "        (\"num\", num_transformer1, [\"age\"])\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "preprocessor2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"encoding\", OneHotEncoder(), [\"gender\", \"device_type\", \"browsing_history\", \"ad_position\", \"time_of_day\"]),\n",
    "        (\"num\", num_transformer2, [\"age\", \"num_visits\"])\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df = preprocessor1.fit_transform(X3_train, y3_train)\n",
    "scaled_df_columns = preprocessor1.get_feature_names_out()\n",
    "\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=scaled_df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_scaled_df = preprocessor2.fit_transform(X3_train, y3_train)\n",
    "# result = result.toarray()\n",
    "non_scaled_df_columns = preprocessor2.get_feature_names_out()\n",
    "\n",
    "non_scaled_df = pd.DataFrame(non_scaled_df, columns=non_scaled_df_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Feature Selection\n",
    "\n",
    "We will look at the correlation of the features to see if we should introduce any feature reduction techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for scaled data\n",
    "correlation_matrix = scaled_df.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Feature Correlation Matrix (Scaled Data)\")\n",
    "plot_show(\"scaled_feature_corr_matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for non-scaled data\n",
    "correlation_matrix = non_scaled_df.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Feature Correlation Matrix (Non-Scaled Data)\")\n",
    "plot_show(\"non_scaled_feature_corr_matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Running Models with Cross Validation\n",
    "\n",
    "**I will be running models on the split datasets from `adclicks3`. If `adclicks3` is not performing well even after tuning and evaluation, we will go back to `adclicks2` and see if the model improves.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pd dataframe to store model metrics\n",
    "metrics_store = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing cross validation\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Logistic regression pipeline\n",
    "logreg_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor2),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Evaluation metrics\n",
    "metrics = {\n",
    "    \"model\" : \"LogisticRegression\",\n",
    "    \"accuracy\" : cross_val_score(logreg_pipeline, X3_train, y3_train, cv=cv, scoring=\"accuracy\").mean(),\n",
    "    \"precision\": cross_val_score(logreg_pipeline, X3_train, y3_train, cv=cv, scoring=\"precision\").mean(),\n",
    "    \"recall\" : cross_val_score(logreg_pipeline, X3_train, y3_train, cv=cv, scoring=\"recall\").mean(),\n",
    "    \"f1\" : cross_val_score(logreg_pipeline, X3_train, y3_train, cv=cv, scoring=\"f1\").mean(),\n",
    "    \"auc\" : cross_val_score(logreg_pipeline, X3_train, y3_train, cv=cv, scoring=\"roc_auc\").mean(),\n",
    "    \"neg_log_loss\" : cross_val_score(logreg_pipeline, X3_train, y3_train, cv=cv, scoring=\"neg_log_loss\").mean()\n",
    "}\n",
    "metrics_store.append(metrics)\n",
    "\n",
    "# Predict\n",
    "y_pred = cross_val_predict(logreg_pipeline, X3_train, y3_train, cv=cv, method=\"predict\")\n",
    "y_prob = cross_val_predict(logreg_pipeline, X3_train, y3_train, cv=cv, method=\"predict_proba\")[:, 1]\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, threshold = roc_curve(y3_train, y_prob)\n",
    "auc = metrics_store[-1][\"auc\"].mean()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color=\"blue\", lw=2, label=f\"ROC Curve (AUC = {auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color='grey', linestyle='--', lw=1)  # Diagonal line\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "metrics = pd.DataFrame(metrics_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# Decision Tree pipeline\n",
    "decision_tree = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor2),\n",
    "    ('classifier', tree.DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Evaluation metrics\n",
    "metrics = {\n",
    "    \"model\" : \"DecisionTree\",\n",
    "    \"accuracy\" : cross_val_score(decision_tree, X3_train, y3_train, cv=5, scoring=\"accuracy\").mean(),\n",
    "    \"precision\": cross_val_score(decision_tree, X3_train, y3_train, cv=5, scoring=\"precision\").mean(),\n",
    "    \"recall\" : cross_val_score(decision_tree, X3_train, y3_train, cv=5, scoring=\"recall\").mean(),\n",
    "    \"f1\" : cross_val_score(decision_tree, X3_train, y3_train, cv=5, scoring=\"f1\").mean(),\n",
    "    \"auc\" : cross_val_score(decision_tree, X3_train, y3_train, cv=5, scoring=\"roc_auc\").mean(),\n",
    "    \"neg_log_loss\" : cross_val_score(decision_tree, X3_train, y3_train, cv=5, scoring=\"neg_log_loss\").mean()\n",
    "}\n",
    "metrics_store.append(metrics)\n",
    "\n",
    "# Predict\n",
    "y_pred = cross_val_predict(decision_tree, X3_train, y3_train, cv=5, method=\"predict\")\n",
    "y_prob = cross_val_predict(decision_tree, X3_train, y3_train, cv=5, method=\"predict_proba\")[:, 1]\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, threshold = roc_curve(y3_train, y_prob)\n",
    "auc = metrics_store[-1][\"auc\"].mean()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color=\"blue\", lw=2, label=f\"ROC Curve (AUC = {auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color='grey', linestyle='--', lw=1)  # Diagonal line\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "metrics = pd.DataFrame(metrics_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
